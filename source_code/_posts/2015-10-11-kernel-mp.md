---
layout: post
title: 内核组件：Multipath TCP
category: 内核组件
tags: [内核、Multipath、TCP]
keywords: 内核、传输优化
description:
---


> 用正确的工具，做正确的事情

## 1 MP介绍

### 1.1 MP基本原理

一个MPTCP连接中可以创建并添加多个子TCP连接，每个子TCP连接使用不同的源IP和目的IP以形成不同的路由传输路径，并且在整个MPTCP会话过程中选择最优的路由路径进行数据传输，避免传统TCP连接单条路由传输路径拥塞而导致整个TCP连接饿死。MP整体架构架构如下图所示：

![MPTCP整体架构图](http://7u2rbh.com1.z0.glb.clouddn.com/paasch1.png)

MP使用多条路由路径进行数据传输如下图所示：

![MPTCP使用多条流进行传输](http://7u2rbh.com1.z0.glb.clouddn.com/paasch4.png)

### 1.2 MP的配置使用

MP功能必须使用新品3.14.19系列的内核，确认MP功能是否已经打开：

	cat /proc/sys/net/mptcp/mptcp_enabled

如果为0，表示没有开启MP功能，开启MP将 该内核文件写1；如果为1表示MP功能已经开启。

只有参与会话的两端都开个MP功能才会建立起MPTCP连接，会话两端的主机都开启MP功能后，对主机之间的数据交互抓包，可以看到三次握手SYN数据包中有Multipath TCP Option，如下图所示：

![MPTCP SYN数据包 Multipath TCP Option](http://7u2rbh.com1.z0.glb.clouddn.com/multipath-syn.png)

	
## 2 MP传输优化测试

MP功能是一种双边传输优化技术，可用于时延高、中间经过的路由节点多的传输优化。

### 2.1 测试环境
	
子squid节点：101.100.190.44（新加坡）， 父squid节点：124.115.210.66（中国电信），子节点到父节点的ping时延：

	[root@FM-31798 home]# ping 124.115.210.66
	PING 124.115.210.66 (124.115.210.66) 56(84) bytes of data.	
	64 bytes from 124.115.210.66: icmp_seq=1 ttl=47 time=111 ms
	64 bytes from 124.115.210.66: icmp_seq=2 ttl=47 time=168 ms
	64 bytes from 124.115.210.66: icmp_seq=3 ttl=47 time=172 ms
	64 bytes from 124.115.210.66: icmp_seq=4 ttl=47 time=198 ms
	64 bytes from 124.115.210.66: icmp_seq=5 ttl=47 time=151 ms
	64 bytes from 124.115.210.66: icmp_seq=6 ttl=47 time=139 ms
	64 bytes from 124.115.210.66: icmp_seq=7 ttl=47 time=198 ms
	64 bytes from 124.115.210.66: icmp_seq=8 ttl=47 time=172 ms
	^C
	--- 124.115.210.66 ping statistics ---
	8 packets transmitted, 8 received, 0% packet loss, time 7026ms
	rtt min/avg/max/mdev = 111.545/164.150/198.894/27.573 ms

### 2.2 测试方案

父子节点使用3.14.19系列内核，父节点上部署web服务，然后在子节点上向父节点 发HTTP 请求下载文件，并记录文件的传输时间，以及状态码等信息，测试过程中循环打开和关闭mp功能，以比较mp打开和关闭后的传输质量， 测试脚本如下：

	#!/bin/bash
	SERVER_PATH='124.115.210.66:4000/down'
	CMD_LINE_TIMEOUT='--connect-timeout 10 -m 600'
	CMD_LINE_OUTPUT='-w %{http_code},%{time_total}'
	RESULT_MPTCP='result_mptcp.log'
	
	FILE_ARRY=(4k 32k 256k 1m 16m 64m)
	
	rm -rf ${RESULT_MPTCP}

	while [ 1 == 1 ]
	do
	
    	for file_size in ${FILE_ARRY[*]}
    	do
        	echo 1 > /proc/sys/net/mptcp/mptcp_enabled
        	echo -n "$(date +%F" "%T), $file_size, mptcp_on, " >> ${RESULT_MPTCP}
        	curl ${SERVER_PATH}/${file_size} -o /dev/null $CMD_LINE_TIMEOUT $CMD_LINE_OUTPUT 1>> ${RESULT_MPTCP}
        	res=$?
        	echo ", ${res}" 1>> ${RESULT_MPTCP}
			
        	echo 0 > /proc/sys/net/mptcp/mptcp_enabled
        	echo -n "$(date +%F" "%T), $file_size, mptcp_off, " >> ${RESULT_MPTCP}
        	curl ${SERVER_PATH}/${file_size} -o /dev/null $CMD_LINE_TIMEOUT $CMD_LINE_OUTPUT 1>> ${RESULT_MPTCP}
        	res=$?
        	echo ", ${res}" 1>> ${RESULT_MPTCP}
			
    	done
		
		
	done


### 2.3 测试结果

64M文件下载 MP传输优化效果对比测试，如下图：

![MPTCP 传输优化效果 64m文件下载](http://7u2rbh.com1.z0.glb.clouddn.com/mp_64m.jpg)

16M文件下载 MP传输优化效果对比测试，如下图：

![MPTCP 传输优化效果 16m文件下载](http://7u2rbh.com1.z0.glb.clouddn.com/mp_16m.jpg)

1M文件下载 MP传输优化效果对比测试，如下图：

![MPTCP 传输优化效果 1m文件下载](http://7u2rbh.com1.z0.glb.clouddn.com/mp_1m.jpg)

256K文件下载 MP传输优化效果对比测试，如下图：

![MPTCP 传输优化效果 256K文件下载](http://7u2rbh.com1.z0.glb.clouddn.com/mp_256k.jpg)

32K文件下载 MP传输优化效果对比测试，如下图：

![MPTCP 传输优化效果 32K文件下载](http://7u2rbh.com1.z0.glb.clouddn.com/mp_32k.jpg)

4K文件下载 MP传输优化效果对比测试，如下图：

![MPTCP 传输优化效果 4K文件下载](http://7u2rbh.com1.z0.glb.clouddn.com/mp_4k.jpg)

测试数据说明及分析：

- 测试过程中，数据下载超时时间设置为600s，以后以上下载时间600s的点表示本次下载超时而异常退出；

- mp功能对传输质量优化有明显的效果，尤其对于大文件的下载；

- mp功能关闭的情况下，大文件下载有很多下载超时的实例，怀疑应该是传输路径上的路由节点基于IP做流控或者基于IP地址做拥塞控制，短时间内源IP或者目的IP相同的大量数据包到达某个路由节点，会导致该路由节点对这类数据包单独做流控或者拥塞控制。



详细测试数据见：[mp功能传输优化效果测试数据](http://172.16.1.131/doc-hub/mp-testing.xlsx)
